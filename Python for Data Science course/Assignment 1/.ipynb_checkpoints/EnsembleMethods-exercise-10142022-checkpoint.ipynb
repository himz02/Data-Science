{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Exercise (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the file `planes.csv` from the data folder as a pandas dataframe called features. The dataset has information about different types of airplanes, including the number of tails, year of production, type, manufacturer, model, number of engines, seat, speed, and engine type. \n",
    "\n",
    "In this exercise, we are going to train a classifier to predict the engine type based on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a brief Exploratory Data Analysis. Display summary statistics, handle missing values (if the number of missing values is less than 100 remove those rows), if needed remove unnecessary columns, check the data distribution, and report if there are correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical features in the dataset into ordinal features using the Ordinal Encoder, use ordinal encoding with the Label Encoder for the column `engine` as target labels, and normalize the data to the range [0,1]. Display the shape of the data and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble\n",
    "\n",
    "Train a hard voting ensemble using k-Nearest Neighbors, SVM, and Logistic Regression, and **compare the accuracy of the ensemble to the individual classifiers**.\n",
    "\n",
    "You can use the default values for the models, you don't need to perform fine-tuning.\n",
    "\n",
    "Differently from the examples in the lecture notes on Ensemble Methods, use a 5-fold cross-validation. This means that you don't need to split the data into train and test sets, as this will be performed by the cross_validate function in scikit-learn. \n",
    "\n",
    "Report the mean and standard distribution of the accuracy scores for the 5-fold cross validation. \n",
    "\n",
    "***Use the same approach with 5-fold cross-validation for all models in this exercise***. \n",
    "\n",
    "You may get warnings for some of the models, so if you wish you can ignore the warnings with the following lines.\n",
    "\n",
    "```\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add SGD and Naive Bayes classifiers to the three models above, and report the results of the hard Voting Ensemble with the five individual models. Are there any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a soft Voting Ensemble using 3 individual models of your choice, and compare the results to the hard Voting Ensemble model. Are the results as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Ensemble\n",
    "\n",
    "Train two Bagging Ensembles, one with bootstrapping and one with pasting, and compare the results. You can use the same hyperparameters as in the examples in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a grid search for the better of these two ensembles, by using the following percentages of used data instances: 10%, 30%, 50%, 80%, and 100%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Ensemble\n",
    "\n",
    "Train Boosting Ensembles using Gradient Boosting, AdaBoost, and XGBoost ensembles, and compare the results. Don't forget to use 5-fold cross-validation with all models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensemble\n",
    "\n",
    "Train a Stacking Ensemble with Random Forest and SVM Classifier as base estimators, and Logistic Regression as the final estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train two other Stacking Ensembles using other classifiers of your choice as the final estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train another Stacking Ensemble with Random Forest and XGBoost as base estimators, and Logistic Regression as the final estimator, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
